{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mq8IuIxVW-e",
        "outputId": "e788e7f0-9421-4593-ae75-39ea3165cb92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x11e9f2430>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data.sampler import Sampler\n",
        "import math\n",
        "import copy\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# import faiss\n",
        "\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BllDgfIse7jz"
      },
      "source": [
        "The model used in this analysis is a simple cnn model with 2 convolutional layer and two fully connected layers. The model is split into features, classifier and top_layer to mimic the architecture used in the original paper. See [here](https://github.com/facebookresearch/deepcluster/tree/master/models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QQTYT7tAWk8T"
      },
      "outputs": [],
      "source": [
        "class SimpleCnn(nn.Module):\n",
        "    \n",
        "    def __init__(self, k=10):\n",
        "        \n",
        "        super(SimpleCnn, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(7*7*16, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.top_layer = nn.Linear(64, k)\n",
        "        self._initialize_weights()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        out = self.features(x)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        if self.top_layer:\n",
        "            out = self.top_layer(out)\n",
        "        return out\n",
        "    \n",
        "    def _initialize_weights(self):\n",
        "        for y, m in enumerate(self.modules()):\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                for i in range(m.out_channels):\n",
        "                    m.weight.data[i].normal_(0, math.sqrt(2. / n))\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "\n",
        "def train_supervised(model, device, train_loader, epoch):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    optimizer = torch.optim.SGD(\n",
        "        filter(lambda x: x.requires_grad, model.parameters()),\n",
        "        lr=0.05,\n",
        "        momentum=0.9,\n",
        "        weight_decay=10**(-5)\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    for e in range(epoch):\n",
        "\n",
        "      epoch_loss = 0.0\n",
        "\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          optimizer.zero_grad()\n",
        "          output = model(data)\n",
        "          loss = criterion(output, target)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          epoch_loss += output.shape[0] * loss.item()\n",
        "\n",
        "      print(\"Epoch Nr: \" + str(e))\n",
        "      print(epoch_loss / len(train_loader.dataset))\n",
        "            \n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "atEkxlBJXGty"
      },
      "outputs": [],
      "source": [
        "# choose device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvWR_NMIfrqL"
      },
      "source": [
        "For simplicity, the whole analysis is done on the mnist dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GfF-9ZXjXSyR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "mnist_train = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "mnist_test = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "\n",
        "# data is splitted in 3 datasets\n",
        "# 1) 55k images - used for unsupervised training \n",
        "# 2) 5k images - used for training of linear calssifier on top of features extracted from network trained with DeepCluster\n",
        "# 3) 10k images - test set\n",
        "\n",
        "unsupervised_pretrain, supervised_train = torch.utils.data.random_split(mnist_train, [55000, 5000])\n",
        "\n",
        "\n",
        "\n",
        "train_loader_unsupervised = torch.utils.data.DataLoader(unsupervised_pretrain, batch_size=64,\n",
        "                                             shuffle=False, num_workers=4)\n",
        "\n",
        "train_loader_supervised = torch.utils.data.DataLoader(supervised_train, batch_size=64,\n",
        "                                             shuffle=False, num_workers=4)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=64,\n",
        "                                             shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Fm2XH3BePi"
      },
      "source": [
        "The following code snippets are taken from the Deepcluster github repository (https://github.com/facebookresearch/deepcluster) an adapted to the task stated above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OfLC6IGMjwvd"
      },
      "outputs": [],
      "source": [
        "def cluster_assign(images_lists, dataset):\n",
        "    \"\"\"Creates a dataset from clustering, with clusters as labels.\n",
        "    Args:\n",
        "        images_lists (list of list): for each cluster, the list of image indexes\n",
        "                                    belonging to this cluster\n",
        "        dataset (list): initial dataset\n",
        "    Returns:\n",
        "        ReassignedDataset(torch.utils.data.Dataset): a dataset with clusters as\n",
        "                                                     labels\n",
        "    \"\"\"\n",
        "    assert images_lists is not None\n",
        "    pseudolabels = []\n",
        "    image_indexes = []\n",
        "    for cluster, images in enumerate(images_lists):\n",
        "        image_indexes.extend(images)\n",
        "        pseudolabels.extend([cluster] * len(images))\n",
        "\n",
        "    t = transforms.Compose([\n",
        "               transforms.ToTensor(),\n",
        "               transforms.Normalize(mean=(0.1307,), std=(0.3081,))]\n",
        "           )\n",
        "\n",
        "    return ReassignedDataset(image_indexes, pseudolabels, dataset, t)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9NT6Kb8lmVW_"
      },
      "outputs": [],
      "source": [
        "class ReassignedDataset(data.Dataset):\n",
        "    \"\"\"A dataset where the new images labels are given in argument. This assigns\n",
        "    each image withits \"pseudolabel\"\n",
        "    Args:\n",
        "        image_indexes (list): list of data indexes\n",
        "        pseudolabels (list): list of labels for each data\n",
        "        dataset (list): list of tuples with paths to images\n",
        "        transform (callable, optional): a function/transform that takes in\n",
        "                                        an PIL image and returns a\n",
        "                                        transformed version\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_indexes, pseudolabels, dataset, transform=None):\n",
        "        self.imgs = self.make_dataset(image_indexes, pseudolabels, dataset)\n",
        "        self.transform = transform\n",
        "\n",
        "    def make_dataset(self, image_indexes, pseudolabels, dataset):\n",
        "        label_to_idx = {label: idx for idx, label in enumerate(set(pseudolabels))}\n",
        "        images = []\n",
        "        for j, idx in enumerate(image_indexes):\n",
        "            path = dataset[idx][0]\n",
        "            pseudolabel = label_to_idx[pseudolabels[j]]\n",
        "            images.append((path, pseudolabel))\n",
        "        return images\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            index (int): index of data\n",
        "        Returns:\n",
        "            tuple: (image, pseudolabel) where pseudolabel is the cluster of index datapoint\n",
        "        \"\"\"\n",
        "        img, pseudolabel = self.imgs[index]\n",
        "        return img, pseudolabel\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JEPuZZ-rn40U"
      },
      "outputs": [],
      "source": [
        "class UnifLabelSampler(Sampler):\n",
        "    \"\"\"Samples elements uniformely accross pseudolabels.\n",
        "        Args:\n",
        "            N (int): size of returned iterator.\n",
        "            images_lists: dict of key (target), value (list of data with this target)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, N, images_lists):\n",
        "        self.N = N\n",
        "        self.images_lists = images_lists\n",
        "        self.indexes = self.generate_indexes_epoch()\n",
        "\n",
        "    def generate_indexes_epoch(self):\n",
        "        nmb_non_empty_clusters = 0\n",
        "        for i in range(len(self.images_lists)):\n",
        "            if len(self.images_lists[i]) != 0:\n",
        "                nmb_non_empty_clusters += 1\n",
        "\n",
        "        size_per_pseudolabel = int(self.N / nmb_non_empty_clusters) + 1\n",
        "        res = np.array([])\n",
        "\n",
        "        for i in range(len(self.images_lists)):\n",
        "            # skip empty clusters\n",
        "            if len(self.images_lists[i]) == 0:\n",
        "                continue\n",
        "            indexes = np.random.choice(\n",
        "                self.images_lists[i],\n",
        "                size_per_pseudolabel,\n",
        "                replace=(len(self.images_lists[i]) <= size_per_pseudolabel)\n",
        "            )\n",
        "            res = np.concatenate((res, indexes))\n",
        "\n",
        "        np.random.shuffle(res)\n",
        "        res = list(res.astype('int'))\n",
        "        if len(res) >= self.N:\n",
        "            return res[:self.N]\n",
        "        res += res[: (self.N - len(res))]\n",
        "        return res\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.indexes)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indexes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JsERBDK2IYlb"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def learning_rate_decay(optimizer, t, lr_0):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        lr = lr_0 / np.sqrt(1 + lr_0 * param_group['weight_decay'] * t)\n",
        "        param_group['lr'] = lr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "V6YXCLvcoJQi"
      },
      "outputs": [],
      "source": [
        "def compute_features(dataloader, model, N, get_labels=False):\n",
        "\n",
        "    model.eval()\n",
        "    labels = []\n",
        "\n",
        "    # discard the label information in the dataloader\n",
        "    for i, (input_tensor, label) in enumerate(dataloader):\n",
        "        input_var = torch.autograd.Variable(input_tensor.cuda(), requires_grad=False)\n",
        "        aux = model(input_var).data.cpu().numpy()\n",
        "\n",
        "        if i == 0:\n",
        "            features = np.zeros((N, aux.shape[1]), dtype='float32')\n",
        "\n",
        "        aux = aux.astype('float32')\n",
        "        if i < len(dataloader) - 1:\n",
        "            features[i * 64: (i + 1) * 64] = aux\n",
        "        else:\n",
        "            # special treatment for final batch\n",
        "            features[i * 64:] = aux\n",
        "\n",
        "        # measure elapsed time\n",
        "\n",
        "        labels.append(label.numpy())\n",
        "\n",
        "    labels = np.concatenate(labels)\n",
        "\n",
        "    if get_labels:\n",
        "      return features, labels\n",
        "    \n",
        "    else:\n",
        "      return features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hBrMDcS4pVJS"
      },
      "outputs": [],
      "source": [
        "def train(loader, model, crit, opt, epoch):\n",
        "    \"\"\"Training of the CNN.\n",
        "        Args:\n",
        "            loader (torch.utils.data.DataLoader): Data loader\n",
        "            model (nn.Module): CNN\n",
        "            crit (torch.nn): loss\n",
        "            opt (torch.optim.SGD): optimizer for every parameters with True\n",
        "                                   requires_grad in model except top layer\n",
        "            epoch (int)\n",
        "    \"\"\"\n",
        "    losses = AverageMeter()\n",
        "    # switch to train mode\n",
        "    model.train()\n",
        "\n",
        "    # create an optimizer for the last fc layer\n",
        "    optimizer_tl = torch.optim.SGD(\n",
        "        model.top_layer.parameters(),\n",
        "        lr=0.01,\n",
        "        weight_decay=10**-5,\n",
        "    )\n",
        "\n",
        "    for i, (input_tensor, target) in enumerate(loader):\n",
        "\n",
        "        target = target.cpu()\n",
        "        input_var = torch.autograd.Variable(input_tensor.cpu())\n",
        "        target_var = torch.autograd.Variable(target)\n",
        "\n",
        "        output = model(input_var)\n",
        "        loss = crit(output, target_var)\n",
        "\n",
        "        # record loss\n",
        "        losses.update(loss.data, input_tensor.size(0))\n",
        "\n",
        "        # compute gradient and do SGD step\n",
        "        opt.zero_grad()\n",
        "        optimizer_tl.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        optimizer_tl.step()\n",
        "\n",
        "    return losses.avg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jkIyA5jLYeSz"
      },
      "outputs": [],
      "source": [
        "def DeepCluster(model, device, train_loader, epoch, k):\n",
        "\n",
        "    fd = int(model.top_layer.weight.size()[1])\n",
        "    model.top_layer = None\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.SGD(\n",
        "        filter(lambda x: x.requires_grad, model.parameters()),\n",
        "        lr=0.05,\n",
        "        momentum=0.9,\n",
        "        weight_decay=10**(-5)\n",
        "    )\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion = criterion.to(device)\n",
        "    #cluster_step\n",
        "\n",
        "\n",
        "    for e in range(epoch):\n",
        "         \n",
        "      model.top_layer = None\n",
        "      model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
        "\n",
        "      features = compute_features(train_loader, model, len(unsupervised_pretrain))\n",
        "\n",
        "       # only 64 dims, so no PCA\n",
        "      pipeline = Pipeline([('scaling', StandardScaler())])\n",
        "      \n",
        "      post_scale = pipeline.fit_transform(features)\n",
        "      post_norm = normalize(post_scale, norm=\"l2\")\n",
        "\n",
        "      n_data, d = post_norm.shape\n",
        "\n",
        "      # faiss implementation of k-means\n",
        "      clus = faiss.Clustering(d, k)\n",
        "      clus.seed = np.random.randint(1234)\n",
        "\n",
        "      clus.niter = 20\n",
        "      clus.max_points_per_centroid = 60000\n",
        "\n",
        "      res = faiss.StandardGpuResources()\n",
        "      flat_config = faiss.GpuIndexFlatConfig()\n",
        "      flat_config.useFloat16 = False\n",
        "      flat_config.device = 0\n",
        "      index = faiss.GpuIndexFlatL2(res, d, flat_config)\n",
        "\n",
        "      #get new cluster labels\n",
        "      clus.train(post_norm, index)\n",
        "      _, I = index.search(post_norm, 1)\n",
        "\n",
        "      labels = np.squeeze(I)\n",
        "\n",
        "      unique, counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "      print(\"Epoch Nr: \" + str(e))\n",
        "\n",
        "      print(\"Overview of cluster assignments:\")\n",
        "      print(dict(zip(unique, counts)))\n",
        "\n",
        "      images_lists = [[] for i in range(k)]\n",
        "      for i in range(len(unsupervised_pretrain)):\n",
        "            images_lists[int(labels[i])].append(i)\n",
        "\n",
        "\n",
        "      # create new dataset from pseudolabels\n",
        "      train_dataset = cluster_assign(images_lists, unsupervised_pretrain)\n",
        "\n",
        "      #print(len(train_dataset))\n",
        "      #print(images_lists)\n",
        "\n",
        "      # sample images from uniform distribution over classes\n",
        "      sampler = UnifLabelSampler(int(1 * len(train_dataset)),\n",
        "                                   images_lists)\n",
        "\n",
        "\n",
        "      train_dataloader = torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=64,\n",
        "            num_workers=4,\n",
        "            sampler=sampler,\n",
        "        )\n",
        "      \n",
        "      # reset last layer\n",
        "      mlp = list(model.classifier.children())\n",
        "      mlp.append(nn.ReLU(inplace=True).cuda())\n",
        "      model.classifier = nn.Sequential(*mlp)\n",
        "      model.top_layer = nn.Linear(fd, k)\n",
        "      model.top_layer.weight.data.normal_(0, 0.01)\n",
        "      model.top_layer.bias.data.zero_()\n",
        "      model.top_layer.cuda()\n",
        "\n",
        "\n",
        "\n",
        "      # train step\n",
        "      torch.set_grad_enabled(True)\n",
        "      loss = train(train_dataloader, model, criterion, optimizer, e)\n",
        "      print(loss.cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "krRxtQXLmOd1"
      },
      "outputs": [],
      "source": [
        "def linear_model(model_base, train_loader, test_loader):\n",
        "\n",
        "  model = copy.deepcopy(model_base)\n",
        "  model.to(device)\n",
        "  model.top_layer = None\n",
        "  model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
        "  features,labels = compute_features(train_loader, model, len(supervised_train), get_labels=True)\n",
        "\n",
        "  clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-5, max_iter =10000))\n",
        "  clf.fit(features, labels)\n",
        "\n",
        "  x_test = []\n",
        "  y_true = []\n",
        "\n",
        "  torch.set_grad_enabled(False)\n",
        "  for idx, (pics, labels) in enumerate(test_loader):\n",
        "    pics = pics.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    features_test = model(pics)\n",
        "    x_test.append(features_test.cpu().numpy())\n",
        "    y_true.append(labels)\n",
        "\n",
        "  x_test = np.concatenate(x_test)\n",
        "  y_true = np.concatenate(y_true)\n",
        "\n",
        "  y_pred = clf.predict(x_test)\n",
        "\n",
        "  print(\"Test Accuracy: \" + str(accuracy_score(y_true, y_pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfzxPTFEmYhq"
      },
      "source": [
        "Train CNN in self supervised manner using deepcluster (55k images) - 5 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWHc9bqzZ-iM",
        "outputId": "f7ddbf12-7cc1-40d6-86d7-e1c6d39b1083"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "Torch not compiled with CUDA enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m simpleCNN \u001b[38;5;241m=\u001b[39m SimpleCnn()\n\u001b[1;32m      2\u001b[0m simpleCNN \u001b[38;5;241m=\u001b[39m simpleCNN\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mDeepCluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimpleCNN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_unsupervised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[27], line 26\u001b[0m, in \u001b[0;36mDeepCluster\u001b[0;34m(model, device, train_loader, epoch, k)\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mtop_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mchildren())[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 26\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munsupervised_pretrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m  \u001b[38;5;66;03m# only 64 dims, so no PCA\u001b[39;00m\n\u001b[1;32m     29\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaling\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler())])\n",
            "Cell \u001b[0;32mIn[19], line 8\u001b[0m, in \u001b[0;36mcompute_features\u001b[0;34m(dataloader, model, N, get_labels)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# discard the label information in the dataloader\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (input_tensor, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> 8\u001b[0m     input_var \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m     aux \u001b[38;5;241m=\u001b[39m model(input_var)\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.12/site-packages/torch/cuda/__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     )\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    297\u001b[0m     )\n",
            "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
          ]
        }
      ],
      "source": [
        "simpleCNN = SimpleCnn()\n",
        "simpleCNN = simpleCNN.to(device)\n",
        "DeepCluster(simpleCNN, device, train_loader_unsupervised, 5, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfyuOMxBnk94"
      },
      "source": [
        "Initialize a model with random weights (baseline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tX6K1QFPmhzy"
      },
      "outputs": [],
      "source": [
        "random_CNN = SimpleCnn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrCWVkd5n0Pl"
      },
      "source": [
        "Initialize a model that is trained in a supervised manner on the 55k train images (should act as an upper bound for the performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzyhHA-unyOf",
        "outputId": "e6f2a21d-5d14-4cda-89a7-40a79ddb9e5b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'SimpleCnn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainCNN \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleCnn\u001b[49m()\n\u001b[1;32m      2\u001b[0m trainCNN \u001b[38;5;241m=\u001b[39m trainCNN\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      3\u001b[0m train_supervised(trainCNN, device, train_loader_unsupervised, \u001b[38;5;241m5\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SimpleCnn' is not defined"
          ]
        }
      ],
      "source": [
        "trainCNN = SimpleCnn()\n",
        "trainCNN = trainCNN.to(device)\n",
        "train_supervised(trainCNN, device, train_loader_unsupervised, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOUkYu3qwKYG"
      },
      "source": [
        "Compare the results of a linear model trained on the features of the fist fc layer (5000 labeled images). Evaluated on test set (10000 images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZb6EQr189G_",
        "outputId": "e03bf1d2-8b41-486c-971c-b988f66e2120"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'linear_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# random weight cnn (lower bound)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlinear_model\u001b[49m(random_CNN, train_loader_supervised, test_loader)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'linear_model' is not defined"
          ]
        }
      ],
      "source": [
        "# random weight cnn (lower bound)\n",
        "linear_model(random_CNN, train_loader_supervised, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7ygVBsH1gek",
        "outputId": "8ece0fbf-b04e-4ff1-b503-91ec4ba26ac4"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'linear_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# cnn trained self supervised\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlinear_model\u001b[49m(simpleCNN, train_loader_supervised, test_loader)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'linear_model' is not defined"
          ]
        }
      ],
      "source": [
        "# cnn trained self supervised\n",
        "linear_model(simpleCNN, train_loader_supervised, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K6DGuqJNbjp",
        "outputId": "1a36b979-745c-4d57-934c-1de47d4c7a86"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'linear_model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# cnn trained supervised\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mlinear_model\u001b[49m(trainCNN, train_loader_supervised, test_loader)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'linear_model' is not defined"
          ]
        }
      ],
      "source": [
        "# cnn trained supervised\n",
        "linear_model(trainCNN, train_loader_supervised, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLlQVZk31_qY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "DeepCluster.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
