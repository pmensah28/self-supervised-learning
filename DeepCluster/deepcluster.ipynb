{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-01T13:39:10.879330Z","iopub.status.busy":"2024-06-01T13:39:10.878461Z","iopub.status.idle":"2024-06-01T13:39:32.275148Z","shell.execute_reply":"2024-06-01T13:39:32.273845Z","shell.execute_reply.started":"2024-06-01T13:39:10.879293Z"},"id":"8Mq8IuIxVW-e","outputId":"ad286997-5286-411d-830e-6fa94c34960d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting faiss-gpu\n","  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n","\u001b[0mInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.2\n"]},{"data":{"text/plain":["<torch._C.Generator at 0x78d1a4c1b270>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.optim.lr_scheduler import StepLR\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler, normalize\n","from sklearn.decomposition import PCA\n","from sklearn.pipeline import Pipeline\n","from sklearn.cluster import KMeans\n","from sklearn.linear_model import LinearRegression\n","import torch.utils.data as data\n","from torch.utils.data.sampler import Sampler\n","import math\n","import copy\n","from sklearn.svm import LinearSVC\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import make_classification\n","from sklearn.metrics import accuracy_score\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","!pip install faiss-gpu\n","import faiss\n","\n","torch.manual_seed(0)"]},{"cell_type":"markdown","metadata":{"id":"BllDgfIse7jz"},"source":["The model used in this analysis is a simple cnn model with 2 convolutional layer and two fully connected layers. The model is split into features, classifier and top_layer to mimic the architecture used in the original paper. See [here](https://github.com/facebookresearch/deepcluster/tree/master/models)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:39:32.277410Z","iopub.status.busy":"2024-06-01T13:39:32.276842Z","iopub.status.idle":"2024-06-01T13:39:32.291737Z","shell.execute_reply":"2024-06-01T13:39:32.290544Z","shell.execute_reply.started":"2024-06-01T13:39:32.277379Z"},"id":"QQTYT7tAWk8T","trusted":true},"outputs":[],"source":["class SimpleCnn(nn.Module):\n","    def __init__(self, k=10):\n","        super(SimpleCnn, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # Change input channels to 3\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(4*4*128, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 64),\n","            nn.ReLU()\n","        )\n","        self.top_layer = nn.Linear(64, k)\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        out = self.features(x)\n","        out = out.view(out.size(0), -1)\n","        out = self.classifier(out)\n","        if self.top_layer:\n","            out = self.top_layer(out)\n","        return out\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:43:20.098806Z","iopub.status.busy":"2024-06-01T13:43:20.098432Z","iopub.status.idle":"2024-06-01T13:43:20.111297Z","shell.execute_reply":"2024-06-01T13:43:20.110361Z","shell.execute_reply.started":"2024-06-01T13:43:20.098774Z"},"trusted":true},"outputs":[],"source":["def train_supervised(model, device, train_loader, epoch):\n","    model.train()\n","    torch.set_grad_enabled(True)\n","\n","    optimizer = torch.optim.SGD(\n","        filter(lambda x: x.requires_grad, model.parameters()),\n","        lr=0.05,\n","        momentum=0.9,\n","        weight_decay=10**(-5)\n","    )\n","\n","    criterion = nn.CrossEntropyLoss()\n","    criterion = criterion.to(device)\n","\n","    for e in range(epoch):\n","\n","      epoch_loss = 0.0\n","\n","      for batch_idx, (data, target) in enumerate(train_loader):\n","          data, target = data.to(device), target.to(device)\n","          optimizer.zero_grad()\n","          output = model(data)\n","          loss = criterion(output, target)\n","          loss.backward()\n","          optimizer.step()\n","          epoch_loss += output.shape[0] * loss.item()\n","\n","      print(\"Epoch: \" + str(e))\n","      print(epoch_loss / len(train_loader.dataset))\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:39:32.293221Z","iopub.status.busy":"2024-06-01T13:39:32.292913Z","iopub.status.idle":"2024-06-01T13:39:32.327239Z","shell.execute_reply":"2024-06-01T13:39:32.326454Z","shell.execute_reply.started":"2024-06-01T13:39:32.293195Z"},"id":"atEkxlBJXGty","trusted":true},"outputs":[],"source":["# choose device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-01T13:39:32.329694Z","iopub.status.busy":"2024-06-01T13:39:32.329417Z","iopub.status.idle":"2024-06-01T13:39:41.948477Z","shell.execute_reply":"2024-06-01T13:39:41.947567Z","shell.execute_reply.started":"2024-06-01T13:39:32.329669Z"},"id":"GfF-9ZXjXSyR","outputId":"8b5f1194-1b77-4068-873d-7e579a36823b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:05<00:00, 29797985.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ../data/cifar-10-python.tar.gz to ../data\n","Files already downloaded and verified\n"]}],"source":["import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader, random_split\n","\n","# Define the transformations for CIFAR-10 dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n","])\n","\n","# Load the CIFAR-10 dataset\n","cifar10_train = datasets.CIFAR10('../data', train=True, download=True, transform=transform)\n","cifar10_test = datasets.CIFAR10('../data', train=False, download=True, transform=transform)\n","\n","# Split the training data into unsupervised pretrain (45k) and supervised train (5k)\n","unsupervised_pretrain, supervised_train = torch.utils.data.random_split(cifar10_train, [45000, 5000])\n","\n","# Create data loaders\n","train_loader_unsupervised = DataLoader(unsupervised_pretrain, batch_size=64, shuffle=False, num_workers=4)\n","train_loader_supervised = DataLoader(supervised_train, batch_size=64, shuffle=False, num_workers=4)\n","test_loader = DataLoader(cifar10_test, batch_size=64, shuffle=True, num_workers=4)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:39:41.950162Z","iopub.status.busy":"2024-06-01T13:39:41.949841Z","iopub.status.idle":"2024-06-01T13:39:41.957348Z","shell.execute_reply":"2024-06-01T13:39:41.956330Z","shell.execute_reply.started":"2024-06-01T13:39:41.950135Z"},"id":"OfLC6IGMjwvd","trusted":true},"outputs":[],"source":["def cluster_assign(images_lists, dataset):\n","    \"\"\"Creates a dataset from clustering, with clusters as labels.\n","    Args:\n","        images_lists (list of list): for each cluster, the list of image indexes\n","                                    belonging to this cluster\n","        dataset (list): initial dataset\n","    Returns:\n","        ReassignedDataset(torch.utils.data.Dataset): a dataset with clusters as\n","                                                     labels\n","    \"\"\"\n","    assert images_lists is not None\n","    pseudolabels = []\n","    image_indexes = []\n","    for cluster, images in enumerate(images_lists):\n","        image_indexes.extend(images)\n","        pseudolabels.extend([cluster] * len(images))\n","\n","    t = transforms.Compose([\n","               transforms.ToTensor(),\n","               transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))]\n","           )\n","\n","    return ReassignedDataset(image_indexes, pseudolabels, dataset, t)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:39:41.958755Z","iopub.status.busy":"2024-06-01T13:39:41.958464Z","iopub.status.idle":"2024-06-01T13:39:41.969058Z","shell.execute_reply":"2024-06-01T13:39:41.968286Z","shell.execute_reply.started":"2024-06-01T13:39:41.958720Z"},"id":"9NT6Kb8lmVW_","trusted":true},"outputs":[],"source":["class Dataset(data.Dataset):\n","    \"\"\"A dataset where the new images labels are given in argument. This assigns\n","    each image withits \"pseudolabel\"\n","    Args:\n","        image_indexes (list): list of data indexes\n","        pseudolabels (list): list of labels for each data\n","        dataset (list): list of tuples with paths to images\n","        transform (callable, optional): a function/transform that takes in\n","                                        an PIL image and returns a\n","                                        transformed version\n","    \"\"\"\n","\n","    def __init__(self, image_indexes, pseudolabels, dataset, transform=None):\n","        self.imgs = self.make_dataset(image_indexes, pseudolabels, dataset)\n","        self.transform = transform\n","\n","    def make_dataset(self, image_indexes, pseudolabels, dataset):\n","        label_to_idx = {label: idx for idx, label in enumerate(set(pseudolabels))}\n","        images = []\n","        for j, idx in enumerate(image_indexes):\n","            path = dataset[idx][0]\n","            pseudolabel = label_to_idx[pseudolabels[j]]\n","            images.append((path, pseudolabel))\n","        return images\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Args:\n","            index (int): index of data\n","        Returns:\n","            tuple: (image, pseudolabel) where pseudolabel is the cluster of index datapoint\n","        \"\"\"\n","        img, pseudolabel = self.imgs[index]\n","        return img, pseudolabel\n","\n","    def __len__(self):\n","        return len(self.imgs)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:39:41.971048Z","iopub.status.busy":"2024-06-01T13:39:41.970218Z","iopub.status.idle":"2024-06-01T13:39:41.983925Z","shell.execute_reply":"2024-06-01T13:39:41.982941Z","shell.execute_reply.started":"2024-06-01T13:39:41.971001Z"},"id":"JEPuZZ-rn40U","trusted":true},"outputs":[],"source":["class Sampler(Sampler):\n","    \"\"\"Samples elements uniformely accross pseudolabels.\n","        Args:\n","            N (int): size of returned iterator.\n","            images_lists: dict of key (target), value (list of data with this target)\n","    \"\"\"\n","\n","    def __init__(self, N, images_lists):\n","        self.N = N\n","        self.images_lists = images_lists\n","        self.indexes = self.generate_indexes_epoch()\n","\n","    def generate_indexes_epoch(self):\n","        nmb_non_empty_clusters = 0\n","        for i in range(len(self.images_lists)):\n","            if len(self.images_lists[i]) != 0:\n","                nmb_non_empty_clusters += 1\n","\n","        size_per_pseudolabel = int(self.N / nmb_non_empty_clusters) + 1\n","        res = np.array([])\n","\n","        for i in range(len(self.images_lists)):\n","            # skip empty clusters\n","            if len(self.images_lists[i]) == 0:\n","                continue\n","            indexes = np.random.choice(\n","                self.images_lists[i],\n","                size_per_pseudolabel,\n","                replace=(len(self.images_lists[i]) <= size_per_pseudolabel)\n","            )\n","            res = np.concatenate((res, indexes))\n","\n","        np.random.shuffle(res)\n","        res = list(res.astype('int'))\n","        if len(res) >= self.N:\n","            return res[:self.N]\n","        res += res[: (self.N - len(res))]\n","        return res\n","\n","    def __iter__(self):\n","        return iter(self.indexes)\n","\n","    def __len__(self):\n","        return len(self.indexes)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:39:41.985423Z","iopub.status.busy":"2024-06-01T13:39:41.985088Z","iopub.status.idle":"2024-06-01T13:39:41.995896Z","shell.execute_reply":"2024-06-01T13:39:41.995085Z","shell.execute_reply.started":"2024-06-01T13:39:41.985391Z"},"id":"JsERBDK2IYlb","trusted":true},"outputs":[],"source":["class ComputeAverage(object):\n","    \"\"\"Computes and stores the average and current value\"\"\"\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","\n","def learning_rate_decay(optimizer, t, lr_0):\n","    for param_group in optimizer.param_groups:\n","        lr = lr_0 / np.sqrt(1 + lr_0 * param_group['weight_decay'] * t)\n","        param_group['lr'] = lr\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:39:41.997368Z","iopub.status.busy":"2024-06-01T13:39:41.997094Z","iopub.status.idle":"2024-06-01T13:39:42.006364Z","shell.execute_reply":"2024-06-01T13:39:42.005391Z","shell.execute_reply.started":"2024-06-01T13:39:41.997344Z"},"id":"V6YXCLvcoJQi","trusted":true},"outputs":[],"source":["def compute_features(dataloader, model, N, get_labels=False):\n","\n","    model.eval()\n","    labels = []\n","\n","    # discard the label information in the dataloader\n","    for i, (input_tensor, label) in enumerate(dataloader):\n","        input_var = torch.autograd.Variable(input_tensor.cuda(), requires_grad=False)\n","        aux = model(input_var).data.cpu().numpy()\n","\n","        if i == 0:\n","            features = np.zeros((N, aux.shape[1]), dtype='float32')\n","\n","        aux = aux.astype('float32')\n","        if i < len(dataloader) - 1:\n","            features[i * 64: (i + 1) * 64] = aux\n","        else:\n","            # special treatment for final batch\n","            features[i * 64:] = aux\n","\n","        # measure elapsed time\n","\n","        labels.append(label.numpy())\n","\n","    labels = np.concatenate(labels)\n","\n","    if get_labels:\n","      return features, labels\n","\n","    else:\n","      return features\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:39:42.009606Z","iopub.status.busy":"2024-06-01T13:39:42.009305Z","iopub.status.idle":"2024-06-01T13:39:42.020005Z","shell.execute_reply":"2024-06-01T13:39:42.019168Z","shell.execute_reply.started":"2024-06-01T13:39:42.009582Z"},"id":"hBrMDcS4pVJS","trusted":true},"outputs":[],"source":["def train(loader, model, crit, opt, epoch):\n","    \"\"\"Training of the CNN.\n","        Args:\n","            loader (torch.utils.data.DataLoader): Data loader\n","            model (nn.Module): CNN\n","            crit (torch.nn): loss\n","            opt (torch.optim.SGD): optimizer for every parameters with True\n","                                   requires_grad in model except top layer\n","            epoch (int)\n","    \"\"\"\n","    losses = AverageMeter()\n","    # switch to train mode\n","    model.train()\n","\n","    # create an optimizer for the last fc layer\n","    optimizer_tl = torch.optim.SGD(\n","        model.top_layer.parameters(),\n","        lr=0.01,\n","        weight_decay=10**-5,\n","    )\n","\n","    for i, (input_tensor, target) in enumerate(loader):\n","\n","        target = target.cuda()\n","        input_var = torch.autograd.Variable(input_tensor.cuda())\n","        target_var = torch.autograd.Variable(target)\n","\n","        output = model(input_var)\n","        loss = crit(output, target_var)\n","\n","        # record loss\n","        losses.update(loss.data, input_tensor.size(0))\n","\n","        # compute gradient and do SGD step\n","        opt.zero_grad()\n","        optimizer_tl.zero_grad()\n","        loss.backward()\n","        opt.step()\n","        optimizer_tl.step()\n","\n","    return losses.avg"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:39:42.021393Z","iopub.status.busy":"2024-06-01T13:39:42.021115Z","iopub.status.idle":"2024-06-01T13:39:42.038729Z","shell.execute_reply":"2024-06-01T13:39:42.037835Z","shell.execute_reply.started":"2024-06-01T13:39:42.021370Z"},"id":"jkIyA5jLYeSz","trusted":true},"outputs":[],"source":["def DeepCluster(model, device, train_loader, epoch, k):\n","\n","    fd = int(model.top_layer.weight.size()[1])\n","    model.top_layer = None\n","\n","    model = model.to(device)\n","\n","\n","    optimizer = torch.optim.SGD(\n","        filter(lambda x: x.requires_grad, model.parameters()),\n","        lr=0.05,\n","        momentum=0.9,\n","        weight_decay=10**(-5)\n","    )\n","\n","    criterion = nn.CrossEntropyLoss()\n","    criterion = criterion.to(device)\n","    #cluster_step\n","\n","\n","    for e in range(epoch):\n","\n","      model.top_layer = None\n","      model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n","\n","      features = compute_features(train_loader, model, len(unsupervised_pretrain))\n","\n","       # only 64 dims, so no PCA\n","      pipeline = Pipeline([('scaling', StandardScaler())])\n","\n","      post_scale = pipeline.fit_transform(features)\n","      post_norm = normalize(post_scale, norm=\"l2\")\n","\n","      n_data, d = post_norm.shape\n","\n","      # faiss implementation of k-means\n","      clus = faiss.Clustering(d, k)\n","      clus.seed = np.random.randint(1234)\n","\n","      clus.niter = 20\n","      clus.max_points_per_centroid = 60000\n","\n","      res = faiss.StandardGpuResources()\n","      flat_config = faiss.GpuIndexFlatConfig()\n","      flat_config.useFloat16 = False\n","      flat_config.device = 0\n","      index = faiss.GpuIndexFlatL2(res, d, flat_config)\n","\n","      #get new cluster labels\n","      clus.train(post_norm, index)\n","      _, I = index.search(post_norm, 1)\n","\n","      labels = np.squeeze(I)\n","\n","      unique, counts = np.unique(labels, return_counts=True)\n","\n","      print(\"Epoch: \" + str(e))\n","\n","      print(\"Overview of cluster assignments:\")\n","      print(dict(zip(unique, counts)))\n","\n","      images_lists = [[] for i in range(k)]\n","      for i in range(len(unsupervised_pretrain)):\n","            images_lists[int(labels[i])].append(i)\n","\n","\n","      # create new dataset from pseudolabels\n","      train_dataset = cluster_assign(images_lists, unsupervised_pretrain)\n","\n","      #print(len(train_dataset))\n","      #print(images_lists)\n","\n","      # sample images from uniform distribution over classes\n","      sampler = UnifLabelSampler(int(1 * len(train_dataset)),\n","                                   images_lists)\n","\n","\n","      train_dataloader = torch.utils.data.DataLoader(\n","            train_dataset,\n","            batch_size=64,\n","            num_workers=4,\n","            sampler=sampler,\n","        )\n","\n","      # reset last layer\n","      mlp = list(model.classifier.children())\n","      mlp.append(nn.ReLU(inplace=True).cuda())\n","      model.classifier = nn.Sequential(*mlp)\n","      model.top_layer = nn.Linear(fd, k)\n","      model.top_layer.weight.data.normal_(0, 0.01)\n","      model.top_layer.bias.data.zero_()\n","      model.top_layer.cuda()\n","\n","\n","\n","      # train step\n","      torch.set_grad_enabled(True)\n","      loss = train(train_dataloader, model, criterion, optimizer, e)\n","      print(loss.cpu().numpy())\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:39:42.040223Z","iopub.status.busy":"2024-06-01T13:39:42.039885Z","iopub.status.idle":"2024-06-01T13:39:42.051165Z","shell.execute_reply":"2024-06-01T13:39:42.050294Z","shell.execute_reply.started":"2024-06-01T13:39:42.040189Z"},"id":"krRxtQXLmOd1","trusted":true},"outputs":[],"source":["def linear_model(model_base, train_loader, test_loader):\n","\n","  model = copy.deepcopy(model_base)\n","  model.to(device)\n","  model.top_layer = None\n","  model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n","  features,labels = compute_features(train_loader, model, len(supervised_train), get_labels=True)\n","\n","  clf = make_pipeline(StandardScaler(),LinearSVC(random_state=0, tol=1e-5, max_iter =10000))\n","  clf.fit(features, labels)\n","\n","  x_test = []\n","  y_true = []\n","\n","  torch.set_grad_enabled(False)\n","  for idx, (pics, labels) in enumerate(test_loader):\n","    pics = pics.to(device)\n","\n","    model.eval()\n","    features_test = model(pics)\n","    x_test.append(features_test.cpu().numpy())\n","    y_true.append(labels)\n","\n","  x_test = np.concatenate(x_test)\n","  y_true = np.concatenate(y_true)\n","\n","  y_pred = clf.predict(x_test)\n","\n","  print(\"Test Accuracy: \" + str(accuracy_score(y_true, y_pred)))"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-01T13:39:42.052989Z","iopub.status.busy":"2024-06-01T13:39:42.052411Z","iopub.status.idle":"2024-06-01T13:41:35.861384Z","shell.execute_reply":"2024-06-01T13:41:35.859874Z","shell.execute_reply.started":"2024-06-01T13:39:42.052957Z"},"id":"jWHc9bqzZ-iM","outputId":"01dd769a-1dd2-4a6e-fc68-6d8633ca6cd6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch Nr: 0\n","Overview of cluster assignments:\n","{0: 4132, 1: 4081, 2: 6232, 3: 3545, 4: 3907, 5: 5203, 6: 4414, 7: 5124, 8: 4764, 9: 3598}\n","0.824921\n","Epoch Nr: 1\n","Overview of cluster assignments:\n","{0: 4314, 1: 3248, 2: 4593, 3: 4675, 4: 5030, 5: 4864, 6: 5834, 7: 3722, 8: 4210, 9: 4510}\n","0.4759902\n","Epoch Nr: 2\n","Overview of cluster assignments:\n","{0: 3862, 1: 5260, 2: 3405, 3: 4047, 4: 3522, 5: 4066, 6: 5144, 7: 4792, 8: 6101, 9: 4801}\n","0.4394555\n","Epoch Nr: 3\n","Overview of cluster assignments:\n","{0: 3335, 1: 5426, 2: 3666, 3: 4564, 4: 3229, 5: 4958, 6: 4328, 7: 6547, 8: 4644, 9: 4303}\n","0.4360714\n","Epoch Nr: 4\n","Overview of cluster assignments:\n","{0: 3515, 1: 3467, 2: 5267, 3: 3051, 4: 6169, 5: 5160, 6: 2871, 7: 3970, 8: 4316, 9: 7214}\n","0.40906253\n"]}],"source":["simpleCNN = SimpleCnn()\n","simpleCNN = simpleCNN.to(device)\n","DeepCluster(simpleCNN, device, train_loader_unsupervised, 5, 10)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:41:35.866643Z","iopub.status.busy":"2024-06-01T13:41:35.866260Z","iopub.status.idle":"2024-06-01T13:41:35.898957Z","shell.execute_reply":"2024-06-01T13:41:35.897922Z","shell.execute_reply.started":"2024-06-01T13:41:35.866608Z"},"id":"tX6K1QFPmhzy","trusted":true},"outputs":[],"source":["random_CNN = SimpleCnn()"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-01T13:43:29.077694Z","iopub.status.busy":"2024-06-01T13:43:29.077278Z","iopub.status.idle":"2024-06-01T13:44:05.010221Z","shell.execute_reply":"2024-06-01T13:44:05.008968Z","shell.execute_reply.started":"2024-06-01T13:43:29.077660Z"},"id":"PzyhHA-unyOf","outputId":"09a749a8-342b-406c-c868-02f1bafc2417","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0\n","1.5247264789157444\n","Epoch: 1\n","1.0250105587853326\n","Epoch: 2\n","0.8021779124789767\n","Epoch: 3\n","0.6645423480365011\n","Epoch: 4\n","0.5722854996667968\n"]}],"source":["trainCNN = SimpleCnn()\n","trainCNN = trainCNN.to(device)\n","train_supervised(trainCNN, device, train_loader_unsupervised, 5)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-06-01T13:45:27.698144Z","iopub.status.busy":"2024-06-01T13:45:27.697635Z","iopub.status.idle":"2024-06-01T13:47:07.380041Z","shell.execute_reply":"2024-06-01T13:47:07.378413Z","shell.execute_reply.started":"2024-06-01T13:45:27.698095Z"},"id":"D7ygVBsH1gek","outputId":"d9ad13cc-d73c-4ea3-85ef-707438710a46","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.3536\n"]}],"source":["# cnn trained self supervised\n","linear_model(simpleCNN, train_loader_supervised, test_loader)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-06-01T13:47:07.391420Z","iopub.status.busy":"2024-06-01T13:47:07.387187Z","iopub.status.idle":"2024-06-01T13:47:40.899555Z","shell.execute_reply":"2024-06-01T13:47:40.897976Z","shell.execute_reply.started":"2024-06-01T13:47:07.391350Z"},"id":"0K6DGuqJNbjp","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test Accuracy: 0.7409\n"]}],"source":["# cnn trained supervised\n","linear_model(trainCNN, train_loader_supervised, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lLlQVZk31_qY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30716,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
